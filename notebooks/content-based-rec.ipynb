{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,  CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import time\n",
    "# wd = %pwd\n",
    "# if wd.split('\\\\')[-1] == 'notebooks':\n",
    "#     %cd ..\n",
    "\n",
    "from coursemate.dataset import Dataset\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Coursera courses...\n",
      "Loading Coursera reviews...\n",
      "Segmenting out students with less than 3 or more than 50 reviews...\n",
      "Setting the train-test split by rating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "174219it [00:10, 17138.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the training and test rating matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "128771it [00:08, 14652.30it/s]\n",
      "45448it [00:03, 14383.03it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset('../data/Coursera_courses.csv', '../data/Coursera.csv', '../data/Coursera_reviews.csv')\n",
    "dataset.set_interaction_counts(3, 50)\n",
    "dataset.set_train_test_split_by_ratings()\n",
    "training_matrix, test_matrix = dataset.get_train_test_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dataset.train_ratings.merge(dataset.df_courses, how='left',on='course_id')\n",
    "test_df = dataset.test_ratings.merge(dataset.df_courses, how='left',on='course_id')\n",
    "users = dataset.student_set.copy(deep=True)\n",
    "courses = dataset.course_set.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128771 45448\n",
      "30719 30719\n",
      "30719 468\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df), len(test_df))\n",
    "print(len(pd.unique(train_df['reviewers'])),len(pd.unique(test_df['reviewers'])))\n",
    "print(len(users), len(courses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def process_skills(skill_text):\n",
    "    skills = set(skill_text.replace(')','').replace('(','').replace('-',' ').lower().split())\n",
    "    return ' '.join(skills)\n",
    "\n",
    "def process_description(description):\n",
    "    description = description.lower()\n",
    "    description = re.sub(r'[^\\w\\s]', '', description)\n",
    "    tokens = word_tokenize(description)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    #tokens = [ps.stem(word) for word in tokens]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def process_reviewers(reviewer):\n",
    "    reviewer = reviewer.lower()\n",
    "    reviewer = re.sub(r'[^\\w\\s]', '', reviewer).replace('by ', '').strip()\n",
    "    return reviewer\n",
    "\n",
    "train_df['reviewers'] = train_df['reviewers'].apply(process_reviewers)\n",
    "test_df['reviewers'] = test_df['reviewers'].apply(process_reviewers)\n",
    "\n",
    "train_df['skills'] = train_df['skills'].apply(process_skills)\n",
    "test_df['skills'] = test_df['skills'].apply(process_skills)\n",
    "\n",
    "train_df['description'] = train_df['description'].apply(process_description)\n",
    "test_df['description'] = test_df['description'].apply(process_description)\n",
    "\n",
    "courses['description'] = courses['description'].apply(process_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_recommendations = 5:\n",
      "Hitrate: 0.23333333333333334, f1-score: 0.09951566951566952\n",
      "Best parameters for highest hitrate: Vectorizer = TfidfVectorizer, n_features = 1000, category = description\n",
      "Best parameters for highest f1 score: Vectorizer = TfidfVectorizer, n_features = 1000, category = description\n",
      "\n",
      "\n",
      "For n_recommendations = 10:\n",
      "Hitrate: 0.3, f1-score: 0.09083842083842084\n",
      "Best parameters for highest hitrate: Vectorizer = TfidfVectorizer, n_features = 10000, category = description\n",
      "Best parameters for highest f1 score: Vectorizer = TfidfVectorizer, n_features = 10000, category = description\n",
      "\n",
      "\n",
      "For n_recommendations = 15:\n",
      "Hitrate: 0.3333333333333333, f1-score: 0.08465390529463755\n",
      "Best parameters for highest hitrate: Vectorizer = TfidfVectorizer, n_features = 1000, category = description\n",
      "Best parameters for highest f1 score: Vectorizer = CountVectorizer, n_features = 1000, category = skills\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comparing tfid vs count, skills vs description\n",
    "def make_recommendations(user_id,n_recommendations,vectorizer,course_vectors,category):\n",
    "    user_reviews = train_df[train_df['reviewers'] == user_id][category]\n",
    "    user_vector = vectorizer.transform(user_reviews)\n",
    "    reviewed_courses = train_df[train_df['reviewers'] == user_id]['course_id'].unique()\n",
    "    most_similar_courses = find_most_similar_courses(user_vector,course_vectors,reviewed_courses,category)[:n_recommendations]\n",
    "    \n",
    "    recommended_courses = []\n",
    "    for course_id, similarity in most_similar_courses:\n",
    "        recommended_courses.append(course_id)\n",
    "\n",
    "    return recommended_courses\n",
    "\n",
    "\n",
    "def find_most_similar_courses(user_vector,course_vectors,user_reviewed_courses,category):   \n",
    "    most_similar_courses = []\n",
    "    for other_course_id in courses.index:\n",
    "        if other_course_id in user_reviewed_courses:\n",
    "            continue\n",
    "\n",
    "        desc = courses[courses.index == other_course_id][category]\n",
    "        if desc.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        desc = desc.iloc[0]\n",
    "        course_vector = course_vectors[other_course_id]\n",
    "        normalized_user_vector = normalize(user_vector)\n",
    "        normalized_course_vector = normalize(course_vector)\n",
    "        similarity = cosine_similarity(normalized_user_vector, normalized_course_vector)\n",
    "        #similarity = cosine_similarity(user_vector, course_vector)\n",
    "        most_similar_courses.append((other_course_id, similarity.mean()))\n",
    "        \n",
    "    most_similar_courses.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return most_similar_courses\n",
    "\n",
    "def evaluate_model(n_users,n_recommendations, Vectorizer,n_features, category):\n",
    "    grouped_df = train_df.groupby('reviewers')['course_id'].count()\n",
    "    filtered_df = grouped_df[grouped_df > 3]\n",
    "    users = filtered_df.index[:n_users]\n",
    "\n",
    "    vectorizer = Vectorizer(max_features=n_features)\n",
    "    vectorizer.fit(courses[category])\n",
    "    \n",
    "    course_vectors = {}\n",
    "    for id,row in courses.iterrows():\n",
    "        course_vectors[id] = vectorizer.transform([row[category]])\n",
    "    \n",
    "            \n",
    "    hitrate,f1,recall,precision,count = 0,0,0,0,0\n",
    "    for user_id in users:\n",
    "        recommendations = make_recommendations(user_id,n_recommendations,vectorizer,course_vectors,category)\n",
    "\n",
    "        # For metric calculating\n",
    "        user_reviewed_courses = test_df[test_df['reviewers'] == user_id]['course_id'].unique()\n",
    "        res = len(set(user_reviewed_courses) & set(recommendations))\n",
    "\n",
    "        if res > 0:\n",
    "            hitrate += 1\n",
    "\n",
    "        all_courses = np.concatenate((recommendations, user_reviewed_courses))\n",
    "        recommended_vector = [1 if course in recommendations else 0 for course in all_courses]\n",
    "        taken_vector = [1 if course in user_reviewed_courses else 0 for course in all_courses]\n",
    "\n",
    "        # Calculate Metrics\n",
    "        f1 += f1_score(taken_vector, recommended_vector)\n",
    "        precision += precision_score(taken_vector, recommended_vector)\n",
    "        recall += recall_score(taken_vector, recommended_vector)\n",
    "\n",
    "        count +=1\n",
    "    #print(f\"Vectorizer: {Vectorizer.__name__} Features: {n}, category: {category}\")\n",
    "    #print(f\"Hit-rate: {(hitrate / count):.3f}, F1: {(f1 / count):.3f}, Precision: {(precision / count):.3f}, Recall: {(recall / count):.3f}\")\n",
    "    return hitrate / count, f1 / count\n",
    "\n",
    "# Testable parameters\n",
    "how_many_users_to_test = 30\n",
    "vectorizers = [TfidfVectorizer,CountVectorizer]\n",
    "n_recommendations_list = [5, 10,15]\n",
    "n_features_list = [1000,10000,]\n",
    "categories = ['skills', 'description']\n",
    "\n",
    "\n",
    "best_params_hitrate = {}\n",
    "best_params_f1 = {}\n",
    "\n",
    "# Initialize dictionaries to keep track of the highest scores for each n_recommendations\n",
    "highest_hitrate = {}\n",
    "highest_f1 = {}\n",
    "\n",
    "\n",
    "# Gridsearch\n",
    "# Test all combinations of parameters\n",
    "for n_recommendations in n_recommendations_list:\n",
    "    # Initialize the highest scores for this n_recommendations\n",
    "    highest_hitrate[n_recommendations] = 0\n",
    "    highest_f1[n_recommendations] = 0\n",
    "\n",
    "    for vectorizer in vectorizers:\n",
    "        for n_features in n_features_list:\n",
    "            for category in categories:\n",
    "                hitrate, f1 = evaluate_model(how_many_users_to_test, n_recommendations, vectorizer, n_features, category)\n",
    "\n",
    "                # Update the best parameters for hitrate\n",
    "                if hitrate > highest_hitrate[n_recommendations]:\n",
    "                    highest_hitrate[n_recommendations] = hitrate\n",
    "                    best_params_hitrate[n_recommendations] = (vectorizer, n_features, category)\n",
    "\n",
    "                # Update the best parameters for f1 score\n",
    "                if f1 > highest_f1[n_recommendations]:\n",
    "                    highest_f1[n_recommendations] = f1\n",
    "                    best_params_f1[n_recommendations] = (vectorizer, n_features, category)\n",
    "\n",
    "    # Print the best parameters for this n_recommendations\n",
    "    print(f\"For n_recommendations = {n_recommendations}:\")\n",
    "    print(f\"Hitrate: {highest_hitrate[n_recommendations]}, f1-score: {highest_f1[n_recommendations]}\")\n",
    "    print(f\"Best parameters for highest hitrate: Vectorizer = {best_params_hitrate[n_recommendations][0].__name__}, n_features = {best_params_hitrate[n_recommendations][1]}, category = {best_params_hitrate[n_recommendations][2]}\")\n",
    "    print(f\"Best parameters for highest f1 score: Vectorizer = {best_params_f1[n_recommendations][0].__name__}, n_features = {best_params_f1[n_recommendations][1]}, category = {best_params_f1[n_recommendations][2]}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Doc2Vec Features: 10000, duration: 482.120\n",
      "Hit-rate: 0.067, F1: 0.020, Precision: 0.012, Recall: 0.056\n"
     ]
    }
   ],
   "source": [
    "# Testing Doc2Vec\n",
    "def make_recommendations(user_id, n_recommendations, model, course_vectors,category):\n",
    "    user_reviews = train_df[train_df['reviewers'] == user_id][category]\n",
    "    user_vector = model.infer_vector(user_reviews).reshape(1, -1)\n",
    "    reviewed_courses = train_df[train_df['reviewers'] == user_id]['course_id'].unique()\n",
    "    most_similar_courses = find_most_similar_courses(user_vector, course_vectors, reviewed_courses,category)[:n_recommendations]\n",
    "    \n",
    "    recommended_courses = []\n",
    "    for course_id, similarity in most_similar_courses:\n",
    "        recommended_courses.append(course_id)\n",
    "\n",
    "    return recommended_courses\n",
    "\n",
    "def find_most_similar_courses(user_vector, course_vectors, user_reviewed_courses,category):   \n",
    "    most_similar_courses = []\n",
    "    for other_course_id in courses.index:\n",
    "        if other_course_id in user_reviewed_courses:\n",
    "            continue\n",
    "\n",
    "        desc = train_df[train_df['course_id'] == other_course_id][category]\n",
    "        if desc.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        desc = desc.iloc[0]\n",
    "        course_vector = course_vectors[other_course_id].reshape(1, -1)\n",
    "\n",
    "        normalized_user_vector = normalize(user_vector)\n",
    "        normalized_course_vector = normalize(course_vector)\n",
    "        similarity = cosine_similarity(normalized_user_vector, normalized_course_vector)\n",
    "        #similarity = cosine_similarity(user_vector, course_vector)\n",
    "        most_similar_courses.append((other_course_id, similarity[0]))\n",
    "        \n",
    "    most_similar_courses.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return most_similar_courses\n",
    "\n",
    "def evaluate_model(n_users, n_recommendations, vectorizer_model, n_features, category):\n",
    "    grouped_df = train_df.groupby('reviewers')['course_id'].count()\n",
    "    filtered_df = grouped_df[grouped_df > 3]\n",
    "    users = filtered_df.index[:n_users]      \n",
    "    start_time = time.time()\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(courses[category].tolist())]\n",
    "\n",
    "    model = vectorizer_model(documents, dm=1, vector_size=n_features, epochs=250)    \n",
    "    vectors = [model.infer_vector(word_tokenize(doc.lower())) for doc in courses['description'].tolist()]\n",
    "\n",
    "    course_vectors = {}\n",
    "    for i, row in enumerate(courses.iterrows()):\n",
    "        course_vectors[row[0]] = vectors[i]\n",
    "\n",
    "    hitrate, f1, recall, precision, count = 0, 0, 0, 0, 0\n",
    "    for user_id in users:\n",
    "        recommendations = make_recommendations(user_id, n_recommendations, model, course_vectors, category)\n",
    "\n",
    "        # For metric calculating\n",
    "        user_reviewed_courses = test_df[test_df['reviewers'] == user_id]['course_id'].unique()\n",
    "        res = len(set(user_reviewed_courses) & set(recommendations))\n",
    "\n",
    "        if res > 0:\n",
    "            hitrate += 1\n",
    "\n",
    "        all_courses = np.concatenate((recommendations, user_reviewed_courses))\n",
    "        recommended_vector = [1 if course in recommendations else 0 for course in all_courses]\n",
    "        taken_vector = [1 if course in user_reviewed_courses else 0 for course in all_courses]\n",
    "\n",
    "        # Calculate Metrics\n",
    "        f1 += f1_score(taken_vector, recommended_vector)\n",
    "        precision += precision_score(taken_vector, recommended_vector)\n",
    "        recall += recall_score(taken_vector, recommended_vector)\n",
    "\n",
    "        count +=1\n",
    "    print(f\"Model: {model.__class__.__name__} Features: {n_features}, duration: {(time.time()-start_time):.3f}\")\n",
    "    print(f\"Hit-rate: {(hitrate / count):.3f}, F1: {(f1 / count):.3f}, Precision: {(precision / count):.3f}, Recall: {(recall / count):.3f}\")\n",
    "evaluate_model(30, 10, Doc2Vec, 10000, 'description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: paraphrase-distilroberta-base-v1 Features: 10000\n",
      "Hit-rate: 0.067, F1: 0.027, Precision: 0.022, Recall: 0.033\n",
      "Model: paraphrase-multilingual-MiniLM-L12-v2 Features: 10000\n",
      "Hit-rate: 0.033, F1: 0.013, Precision: 0.011, Recall: 0.017\n",
      "Model: distiluse-base-multilingual-cased-v2 Features: 10000\n",
      "Hit-rate: 0.067, F1: 0.028, Precision: 0.022, Recall: 0.039\n",
      "Model: all-mpnet-base-v2 Features: 10000\n",
      "Hit-rate: 0.033, F1: 0.013, Precision: 0.011, Recall: 0.017\n",
      "Model: paraphrase-distilroberta-base-v1 Features: 10000\n",
      "Hit-rate: 0.100, F1: 0.043, Precision: 0.033, Recall: 0.061\n",
      "Model: paraphrase-multilingual-MiniLM-L12-v2 Features: 10000\n",
      "Hit-rate: 0.133, F1: 0.055, Precision: 0.044, Recall: 0.074\n",
      "Model: distiluse-base-multilingual-cased-v2 Features: 10000\n",
      "Hit-rate: 0.133, F1: 0.055, Precision: 0.044, Recall: 0.074\n",
      "Model: all-mpnet-base-v2 Features: 10000\n",
      "Hit-rate: 0.067, F1: 0.030, Precision: 0.022, Recall: 0.044\n"
     ]
    }
   ],
   "source": [
    "# Testing SentenceTransformers\n",
    "def make_recommendations(user_id, n_recommendations, model, course_vectors, category):\n",
    "    user_reviews = train_df[train_df['reviewers'] == user_id][category]\n",
    "    user_vector = model.encode(user_reviews.tolist())\n",
    "    reviewed_courses = train_df[train_df['reviewers'] == user_id]['course_id'].unique()\n",
    "    most_similar_courses = find_most_similar_courses(user_vector, course_vectors, reviewed_courses, category)[:n_recommendations]\n",
    "\n",
    "    recommended_courses = [course_id for course_id, _ in most_similar_courses]\n",
    "\n",
    "    return recommended_courses\n",
    "\n",
    "\n",
    "def find_most_similar_courses(user_vector, course_vectors, user_reviewed_courses, category):\n",
    "    most_similar_courses = []\n",
    "    for other_course_id in courses.index:\n",
    "        if other_course_id in user_reviewed_courses:\n",
    "            continue\n",
    "\n",
    "        desc = train_df[train_df['course_id'] == other_course_id][category]\n",
    "        if desc.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        desc = desc.iloc[0]\n",
    "        course_vector = course_vectors[other_course_id]  # Use precomputed vectors\n",
    "        similarity = cosine_similarity(user_vector, [course_vector])[0][0]  # Remove the extra list\n",
    "        most_similar_courses.append((other_course_id, similarity))\n",
    "\n",
    "    most_similar_courses.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return most_similar_courses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(n_users, n_recommendations, model_name, category):\n",
    "    grouped_df = train_df.groupby('reviewers')['course_id'].count()\n",
    "    filtered_df = grouped_df[grouped_df > 3]\n",
    "    users = filtered_df.index[:n_users]\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "    course_vectors = {}\n",
    "    for id, row in courses.iterrows():\n",
    "        course_vectors[id] = model.encode([row[category]])[0]\n",
    "\n",
    "    hitrate, f1, recall, precision, count = 0, 0, 0, 0, 0\n",
    "    for user_id in users:\n",
    "        recommendations = make_recommendations(user_id, n_recommendations, model, course_vectors, category)\n",
    "\n",
    "        # For metric calculating\n",
    "        user_reviewed_courses = test_df[test_df['reviewers'] == user_id]['course_id'].unique()\n",
    "        res = len(set(user_reviewed_courses) & set(recommendations))\n",
    "\n",
    "        if res > 0:\n",
    "            hitrate += 1\n",
    "\n",
    "        all_courses = np.concatenate((recommendations, user_reviewed_courses))\n",
    "        recommended_vector = [1 if course in recommendations else 0 for course in all_courses]\n",
    "        taken_vector = [1 if course in user_reviewed_courses else 0 for course in all_courses]\n",
    "\n",
    "        # Calculate Metrics\n",
    "        f1 += f1_score(taken_vector, recommended_vector)\n",
    "        precision += precision_score(taken_vector, recommended_vector)\n",
    "        recall += recall_score(taken_vector, recommended_vector)\n",
    "\n",
    "        count +=1\n",
    "    print(f\"Model: {model_name} Features: {n_features}\")\n",
    "    print(f\"Hit-rate: {(hitrate / count):.3f}, F1: {(f1 / count):.3f}, Precision: {(precision / count):.3f}, Recall: {(recall / count):.3f}\")\n",
    "\n",
    "\n",
    "evaluate_model(30, 5, 'paraphrase-distilroberta-base-v1','description')\n",
    "evaluate_model(30, 5, 'paraphrase-multilingual-MiniLM-L12-v2','description')\n",
    "evaluate_model(30, 5, 'distiluse-base-multilingual-cased-v2','description')\n",
    "evaluate_model(30, 5, 'all-mpnet-base-v2','description')\n",
    "\n",
    "evaluate_model(30, 5, 'paraphrase-distilroberta-base-v1','skills')\n",
    "evaluate_model(30, 5, 'paraphrase-multilingual-MiniLM-L12-v2','skills')\n",
    "evaluate_model(30, 5, 'distiluse-base-multilingual-cased-v2','skills')\n",
    "evaluate_model(30, 5, 'all-mpnet-base-v2','skills')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['', '18it042 c j', '2516_anirudh g', 'a', 'a a a', 'a c', 'a e h',\n",
      "       'a f m m h b', 'a k', 'a p'],\n",
      "      dtype='object', name='reviewers')\n"
     ]
    }
   ],
   "source": [
    "grouped_df = train_df.groupby('reviewers')['course_id'].count()\n",
    "filtered_df = grouped_df[grouped_df > 3]\n",
    "users = filtered_df.index[:10]\n",
    "print(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_recommendations = 5:\n",
      "Hitrate: 0.18, f1-score: 0.07637606837606838\n",
      "Best parameters for highest hitrate: Vectorizer = TfidfVectorizer, n_features = 10000\n",
      "Best parameters for highest f1 score: Vectorizer = TfidfVectorizer, n_features = 10000\n",
      "\n",
      "\n",
      "For n_recommendations = 10:\n",
      "Hitrate: 0.26, f1-score: 0.08407795128847761\n",
      "Best parameters for highest hitrate: Vectorizer = TfidfVectorizer, n_features = 10000\n",
      "Best parameters for highest f1 score: Vectorizer = TfidfVectorizer, n_features = 10000\n",
      "\n",
      "\n",
      "For n_recommendations = 15:\n",
      "Hitrate: 0.32, f1-score: 0.07877316479604811\n",
      "Best parameters for highest hitrate: Vectorizer = TfidfVectorizer, n_features = 10000\n",
      "Best parameters for highest f1 score: Vectorizer = TfidfVectorizer, n_features = 10000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing Count vs Tfid on skills combined with description\n",
    "def make_recommendations(user_id,n_recommendations,vectorizer,course_vectors):\n",
    "    user_reviews_skills = train_df[train_df['reviewers'] == user_id]['skills']\n",
    "    user_reviews_description = train_df[train_df['reviewers'] == user_id]['description']\n",
    "    user_reviews_combined = user_reviews_skills + ' ' + user_reviews_description\n",
    "    user_vector = vectorizer.transform(user_reviews_combined)\n",
    "    reviewed_courses = train_df[train_df['reviewers'] == user_id]['course_id'].unique()\n",
    "\n",
    "    most_similar_courses = find_most_similar_courses(user_vector,course_vectors,reviewed_courses)[:n_recommendations]\n",
    "    \n",
    "    recommended_courses = []\n",
    "    for course_id, similarity in most_similar_courses:\n",
    "        recommended_courses.append(course_id)\n",
    "\n",
    "    return recommended_courses\n",
    "\n",
    "\n",
    "def find_most_similar_courses(user_vector,course_vectors,user_reviewed_courses):   \n",
    "    most_similar_courses = []\n",
    "    for other_course_id in courses.index:\n",
    "        if other_course_id in user_reviewed_courses:\n",
    "            continue\n",
    "\n",
    "        course_vector = course_vectors[other_course_id]\n",
    "        normalized_user_vector = normalize(user_vector)\n",
    "        normalized_course_vector = normalize(course_vector)\n",
    "        similarity = cosine_similarity(normalized_user_vector, normalized_course_vector)\n",
    "        most_similar_courses.append((other_course_id, similarity.mean()))\n",
    "        \n",
    "    most_similar_courses.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return most_similar_courses\n",
    "\n",
    "def evaluate_model(n_users,n_recommendations, Vectorizer,n_features, category):\n",
    "    grouped_df = train_df.groupby('reviewers')['course_id'].count()\n",
    "    filtered_df = grouped_df[grouped_df > 3]\n",
    "    users = filtered_df.index[:n_users]\n",
    "\n",
    "    vectorizer = Vectorizer(max_features=n_features)\n",
    "    vectorizer.fit(courses['description']+courses['skills'])\n",
    "    \n",
    "    course_vectors = {}\n",
    "    for id,row in courses.iterrows():\n",
    "        course_vectors[id] = vectorizer.transform([row['description'] + row['skills']])\n",
    "    \n",
    "\n",
    "    hitrate,f1,recall,precision,count = 0,0,0,0,0\n",
    "    for user_id in users:\n",
    "        recommendations = make_recommendations(user_id,n_recommendations,vectorizer,course_vectors)\n",
    "\n",
    "        # For metric calculating\n",
    "        user_reviewed_courses = test_df[test_df['reviewers'] == user_id]['course_id'].unique()\n",
    "        res = len(set(user_reviewed_courses) & set(recommendations))\n",
    "\n",
    "        if res > 0:\n",
    "            hitrate += 1\n",
    "\n",
    "        all_courses = np.concatenate((recommendations, user_reviewed_courses))\n",
    "        recommended_vector = [1 if course in recommendations else 0 for course in all_courses]\n",
    "        taken_vector = [1 if course in user_reviewed_courses else 0 for course in all_courses]\n",
    "\n",
    "        # Calculate Metrics\n",
    "        f1 += f1_score(taken_vector, recommended_vector)\n",
    "        precision += precision_score(taken_vector, recommended_vector)\n",
    "\n",
    "        count +=1\n",
    "\n",
    "    return hitrate / count, f1 / count\n",
    "\n",
    "# Testable parameters\n",
    "how_many_users_to_test = 50\n",
    "vectorizers = [TfidfVectorizer]\n",
    "n_recommendations_list = [5, 10,15]\n",
    "n_features_list = [10000]\n",
    "categories = ['skills', 'description']\n",
    "\n",
    "\n",
    "best_params_hitrate = {}\n",
    "best_params_f1 = {}\n",
    "\n",
    "# Initialize dictionaries to keep track of the highest scores for each n_recommendations\n",
    "highest_hitrate = {}\n",
    "highest_f1 = {}\n",
    "\n",
    "\n",
    "# # Gridsearch\n",
    "# Test all combinations of parameters\n",
    "for n_recommendations in n_recommendations_list:\n",
    "    # Initialize the highest scores for this n_recommendations\n",
    "    highest_hitrate[n_recommendations] = 0\n",
    "    highest_f1[n_recommendations] = 0\n",
    "\n",
    "    for vectorizer in vectorizers:\n",
    "        for n_features in n_features_list:\n",
    "            for category in categories:\n",
    "                hitrate, f1 = evaluate_model(how_many_users_to_test, n_recommendations, vectorizer, n_features, category)\n",
    "\n",
    "                # Update the best parameters for hitrate\n",
    "                if hitrate > highest_hitrate[n_recommendations]:\n",
    "                    highest_hitrate[n_recommendations] = hitrate\n",
    "                    best_params_hitrate[n_recommendations] = (vectorizer, n_features, category)\n",
    "\n",
    "                # Update the best parameters for f1 score\n",
    "                if f1 > highest_f1[n_recommendations]:\n",
    "                    highest_f1[n_recommendations] = f1\n",
    "                    best_params_f1[n_recommendations] = (vectorizer, n_features, category)\n",
    "\n",
    "    # Print the best parameters for this n_recommendations\n",
    "    print(f\"For n_recommendations = {n_recommendations}:\")\n",
    "    print(f\"Hitrate: {highest_hitrate[n_recommendations]}, f1-score: {highest_f1[n_recommendations]}\")\n",
    "    print(f\"Best parameters for highest hitrate: Vectorizer = {best_params_hitrate[n_recommendations][0].__name__}, n_features = {best_params_hitrate[n_recommendations][1]}\")\n",
    "    print(f\"Best parameters for highest f1 score: Vectorizer = {best_params_f1[n_recommendations][0].__name__}, n_features = {best_params_f1[n_recommendations][1]}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neural-networks-deep-learning' 'deep-neural-network'\n",
      " 'machine-learning-projects' 'nlp-sequence-models']\n",
      "recommended courses ['introduction-tensorflow', 'natural-language-processing-tensorflow', 'convolutional-neural-networks-tensorflow', 'sequence-models-in-nlp', 'tensorflow-sequences-time-series-and-prediction', 'attention-models-in-nlp', 'getting-started-with-tensor-flow2', 'convolutional-neural-networks', 'introduction-to-ai', 'ai-for-medical-prognosis']\n"
     ]
    }
   ],
   "source": [
    "# Testing 1\n",
    "def make_recommendations(user_id,n_recommendations,vectorizer,course_vectors):\n",
    "    user_reviews_skills = train_df[train_df['reviewers'] == user_id]['skills']\n",
    "    user_reviews_description = train_df[train_df['reviewers'] == user_id]['description']\n",
    "    user_reviews_combined = user_reviews_skills + ' ' + user_reviews_description\n",
    "    user_vector = vectorizer.transform(user_reviews_combined)\n",
    "    reviewed_courses = train_df[train_df['reviewers'] == user_id]['course_id'].unique()\n",
    "\n",
    "    most_similar_courses = find_most_similar_courses(user_vector,course_vectors,reviewed_courses)[:n_recommendations]\n",
    "    \n",
    "    recommended_courses = []\n",
    "    for course_id, similarity in most_similar_courses:\n",
    "        recommended_courses.append(course_id)\n",
    "\n",
    "    return recommended_courses\n",
    "\n",
    "\n",
    "def find_most_similar_courses(user_vector,course_vectors,user_reviewed_courses):   \n",
    "    most_similar_courses = []\n",
    "    print(user_reviewed_courses)\n",
    "    for other_course_id in courses.index:\n",
    "        if other_course_id in user_reviewed_courses:\n",
    "            continue\n",
    "\n",
    "        course_vector = course_vectors[other_course_id]\n",
    "        normalized_user_vector = normalize(user_vector)\n",
    "        normalized_course_vector = normalize(course_vector)\n",
    "        similarity = cosine_similarity(normalized_user_vector, normalized_course_vector)\n",
    "        most_similar_courses.append((other_course_id, similarity.mean()))\n",
    "        \n",
    "    most_similar_courses.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return most_similar_courses\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "vectorizer.fit(courses['description']+courses['skills'])\n",
    "\n",
    "course_vectors = {}\n",
    "for id,row in courses.iterrows():\n",
    "    course_vectors[id] = vectorizer.transform([row['description'] + row['skills']])\n",
    "recs = make_recommendations('18it042 c j',10,vectorizer,course_vectors)\n",
    "print(\"recommended courses\", recs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
