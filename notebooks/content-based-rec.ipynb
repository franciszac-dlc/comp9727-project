{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import time\n",
    "wd = %pwd\n",
    "if wd.split('\\\\')[-1] == 'notebooks':\n",
    "    %cd ..\n",
    "\n",
    "from coursemate.dataset import Dataset\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Coursera courses...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Coursera reviews...\n",
      "Segmenting out students with less than 3 or more than 50 reviews...\n",
      "Setting the train-test split by rating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "174219it [00:10, 16201.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the training and test rating matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "128771it [00:09, 13868.40it/s]\n",
      "45448it [00:03, 13526.30it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset('data/Coursera_courses.csv', 'data/Coursera.csv', 'data/Coursera_reviews.csv')\n",
    "dataset.set_interaction_counts(3, 50)\n",
    "dataset.set_train_test_split_by_ratings(ratio=0.8)\n",
    "training_matrix, test_matrix = dataset.get_train_test_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dataset.train_ratings.merge(dataset.df_courses, how='left',on='course_id')\n",
    "test_df = dataset.test_ratings.merge(dataset.df_courses, how='left',on='course_id')\n",
    "users = dataset.student_set.copy(deep=True)\n",
    "courses = dataset.course_set.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128771 45448\n",
      "30719 30719\n",
      "30719 468\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df), len(test_df))\n",
    "print(len(pd.unique(train_df['reviewers'])),len(pd.unique(test_df['reviewers'])))\n",
    "print(len(users), len(courses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def process_skills(skill_text):\n",
    "    skills = set(skill_text.replace(')','').replace('(','').replace('-',' ').lower().split())\n",
    "    return ' '.join(skills)\n",
    "\n",
    "def process_description(description):\n",
    "    description = description.lower()\n",
    "    description = re.sub(r'[^\\w\\s]', '', description)\n",
    "    tokens = word_tokenize(description)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    #tokens = [ps.stem(word) for word in tokens]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def process_reviewers(reviewer):\n",
    "    reviewer = reviewer.lower()\n",
    "    reviewer = re.sub(r'[^\\w\\s]', '', reviewer).replace('by ', '').strip()\n",
    "    return reviewer\n",
    "\n",
    "train_df['reviewers'] = train_df['reviewers'].apply(process_reviewers)\n",
    "test_df['reviewers'] = test_df['reviewers'].apply(process_reviewers)\n",
    "\n",
    "train_df['skills'] = train_df['skills'].apply(process_skills)\n",
    "test_df['skills'] = test_df['skills'].apply(process_skills)\n",
    "\n",
    "train_df['description'] = train_df['description'].apply(process_description)\n",
    "test_df['description'] = test_df['description'].apply(process_description)\n",
    "\n",
    "courses['description'] = courses['description'].apply(process_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_recommendations = 5:\n",
      "Hitrate: 0.2, f1-score: 0.08958300958300956\n",
      "Best parameters for highest hitrate: Vectorizer = TfidfVectorizer, n_features = 1000, category = description\n",
      "Best parameters for highest f1 score: Vectorizer = TfidfVectorizer, n_features = 1000, category = description\n",
      "\n",
      "\n",
      "For n_recommendations = 10:\n",
      "Hitrate: 0.36666666666666664, f1-score: 0.1148580286815581\n",
      "Best parameters for highest hitrate: Vectorizer = TfidfVectorizer, n_features = 10000, category = description\n",
      "Best parameters for highest f1 score: Vectorizer = TfidfVectorizer, n_features = 10000, category = description\n",
      "\n",
      "\n",
      "For n_recommendations = 15:\n",
      "Hitrate: 0.4, f1-score: 0.10369570390623023\n",
      "Best parameters for highest hitrate: Vectorizer = TfidfVectorizer, n_features = 10000, category = description\n",
      "Best parameters for highest f1 score: Vectorizer = TfidfVectorizer, n_features = 10000, category = description\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def make_recommendations(user_id,n_recommendations,vectorizer,course_vectors,category):\n",
    "    user_reviews = train_df[train_df['reviewers'] == user_id][category]\n",
    "    user_vector = vectorizer.transform(user_reviews)\n",
    "\n",
    "    most_similar_courses = find_most_similar_courses(user_vector,course_vectors,user_reviews,category)[:n_recommendations]\n",
    "    \n",
    "    recommended_courses = []\n",
    "    for course_id, similarity in most_similar_courses:\n",
    "        recommended_courses.append(course_id)\n",
    "\n",
    "    return recommended_courses\n",
    "\n",
    "\n",
    "def find_most_similar_courses(user_vector,course_vectors,user_reviewed_courses,category):   \n",
    "    most_similar_courses = []\n",
    "    for other_course_id in courses.index:\n",
    "        if other_course_id in user_reviewed_courses:\n",
    "            continue\n",
    "\n",
    "        desc = train_df[train_df['course_id'] == other_course_id][category]\n",
    "        if desc.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        desc = desc.iloc[0]\n",
    "        course_vector = course_vectors[other_course_id]\n",
    "        normalized_user_vector = normalize(user_vector)\n",
    "        normalized_course_vector = normalize(course_vector)\n",
    "        similarity = cosine_similarity(normalized_user_vector, normalized_course_vector)\n",
    "        #similarity = cosine_similarity(user_vector, course_vector)\n",
    "        most_similar_courses.append((other_course_id, similarity.mean()))\n",
    "        \n",
    "    most_similar_courses.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return most_similar_courses\n",
    "\n",
    "def evaluate_model(n_users,n_recommendations, Vectorizer,n_features, category):\n",
    "    grouped_df = train_df.groupby('reviewers')['course_id'].count()\n",
    "    filtered_df = grouped_df[grouped_df > 3]\n",
    "    users = filtered_df.index[:n_users]\n",
    "\n",
    "    vectorizer = Vectorizer(max_features=n_features)\n",
    "    vectorizer.fit(courses[category])\n",
    "    \n",
    "    course_vectors = {}\n",
    "    for id,row in courses.iterrows():\n",
    "        course_vectors[id] = vectorizer.transform([row[category]])\n",
    "    \n",
    "            \n",
    "    hitrate,f1,recall,precision,count = 0,0,0,0,0\n",
    "    for user_id in users:\n",
    "        recommendations = make_recommendations(user_id,n_recommendations,vectorizer,course_vectors,category)\n",
    "\n",
    "        # For metric calculating\n",
    "        user_reviewed_courses = test_df[test_df['reviewers'] == user_id]['course_id'].unique()\n",
    "        res = len(set(user_reviewed_courses) & set(recommendations))\n",
    "\n",
    "        if res > 0:\n",
    "            hitrate += 1\n",
    "\n",
    "        all_courses = np.concatenate((recommendations, user_reviewed_courses))\n",
    "        recommended_vector = [1 if course in recommendations else 0 for course in all_courses]\n",
    "        taken_vector = [1 if course in user_reviewed_courses else 0 for course in all_courses]\n",
    "\n",
    "        # Calculate Metrics\n",
    "        f1 += f1_score(taken_vector, recommended_vector)\n",
    "        precision += precision_score(taken_vector, recommended_vector)\n",
    "        recall += recall_score(taken_vector, recommended_vector)\n",
    "\n",
    "        count +=1\n",
    "    #print(f\"Vectorizer: {Vectorizer.__name__} Features: {n}, category: {category}\")\n",
    "    #print(f\"Hit-rate: {(hitrate / count):.3f}, F1: {(f1 / count):.3f}, Precision: {(precision / count):.3f}, Recall: {(recall / count):.3f}\")\n",
    "    return hitrate / count, f1 / count\n",
    "\n",
    "# Testable parameters\n",
    "how_many_users_to_test = 30\n",
    "vectorizers = [TfidfVectorizer]\n",
    "n_recommendations_list = [5, 10,15]\n",
    "n_features_list = [1000,10000,100000]\n",
    "categories = ['skills', 'description']\n",
    "\n",
    "\n",
    "best_params_hitrate = {}\n",
    "best_params_f1 = {}\n",
    "\n",
    "# Initialize dictionaries to keep track of the highest scores for each n_recommendations\n",
    "highest_hitrate = {}\n",
    "highest_f1 = {}\n",
    "\n",
    "\n",
    "\n",
    "# Gridsearch\n",
    "# Test all combinations of parameters\n",
    "for n_recommendations in n_recommendations_list:\n",
    "    # Initialize the highest scores for this n_recommendations\n",
    "    highest_hitrate[n_recommendations] = 0\n",
    "    highest_f1[n_recommendations] = 0\n",
    "\n",
    "    for vectorizer in vectorizers:\n",
    "        for n_features in n_features_list:\n",
    "            for category in categories:\n",
    "                hitrate, f1 = evaluate_model(how_many_users_to_test, n_recommendations, vectorizer, n_features, category)\n",
    "\n",
    "                # Update the best parameters for hitrate\n",
    "                if hitrate > highest_hitrate[n_recommendations]:\n",
    "                    highest_hitrate[n_recommendations] = hitrate\n",
    "                    best_params_hitrate[n_recommendations] = (vectorizer, n_features, category)\n",
    "\n",
    "                # Update the best parameters for f1 score\n",
    "                if f1 > highest_f1[n_recommendations]:\n",
    "                    highest_f1[n_recommendations] = f1\n",
    "                    best_params_f1[n_recommendations] = (vectorizer, n_features, category)\n",
    "\n",
    "    # Print the best parameters for this n_recommendations\n",
    "    print(f\"For n_recommendations = {n_recommendations}:\")\n",
    "    print(f\"Hitrate: {highest_hitrate[n_recommendations]}, f1-score: {highest_f1[n_recommendations]}\")\n",
    "    print(f\"Best parameters for highest hitrate: Vectorizer = {best_params_hitrate[n_recommendations][0].__name__}, n_features = {best_params_hitrate[n_recommendations][1]}, category = {best_params_hitrate[n_recommendations][2]}\")\n",
    "    print(f\"Best parameters for highest f1 score: Vectorizer = {best_params_f1[n_recommendations][0].__name__}, n_features = {best_params_f1[n_recommendations][1]}, category = {best_params_f1[n_recommendations][2]}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Doc2Vec Features: 10000, duration: 453.285\n",
      "Hit-rate: 0.000, F1: 0.000, Precision: 0.000, Recall: 0.000\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "def make_recommendations(user_id, n_recommendations, model, course_vectors,category):\n",
    "    user_reviews = train_df[train_df['reviewers'] == user_id][category]\n",
    "    user_vector = model.infer_vector(user_reviews).reshape(1, -1)\n",
    "\n",
    "    #print(train_df[train_df['reviewers'] == user_id]['course_id'].unique())\n",
    "    most_similar_courses = find_most_similar_courses(user_vector, course_vectors, user_reviews,category)[:n_recommendations]\n",
    "    \n",
    "    recommended_courses = []\n",
    "    for course_id, similarity in most_similar_courses:\n",
    "        #print(course_id)\n",
    "        recommended_courses.append(course_id)\n",
    "\n",
    "    #print('\\n\\n')\n",
    "\n",
    "    return recommended_courses\n",
    "\n",
    "def find_most_similar_courses(user_vector, course_vectors, user_reviewed_courses,category):   \n",
    "    most_similar_courses = []\n",
    "    for other_course_id in courses.index:\n",
    "        if other_course_id in user_reviewed_courses:\n",
    "            continue\n",
    "\n",
    "        desc = train_df[train_df['course_id'] == other_course_id][category]\n",
    "        if desc.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        desc = desc.iloc[0]\n",
    "        course_vector = course_vectors[other_course_id].reshape(1, -1)\n",
    "\n",
    "        normalized_user_vector = normalize(user_vector)\n",
    "        normalized_course_vector = normalize(course_vector)\n",
    "        similarity = cosine_similarity(normalized_user_vector, normalized_course_vector)\n",
    "        #similarity = cosine_similarity(user_vector, course_vector)\n",
    "        most_similar_courses.append((other_course_id, similarity[0]))\n",
    "        \n",
    "    most_similar_courses.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return most_similar_courses\n",
    "\n",
    "def evaluate_model(n_users, n_recommendations, vectorizer_model, n_features, category):\n",
    "    grouped_df = train_df.groupby('reviewers')['course_id'].count()\n",
    "    filtered_df = grouped_df[grouped_df > 3]\n",
    "    users = filtered_df.index[:n_users]      \n",
    "    start_time = time.time()\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(courses[category].tolist())]\n",
    "\n",
    "    model = vectorizer_model(documents, dm=1, vector_size=n_features, epochs=250)    \n",
    "    vectors = [model.infer_vector(word_tokenize(doc.lower())) for doc in courses['description'].tolist()]\n",
    "\n",
    "    course_vectors = {}\n",
    "    for i, row in enumerate(courses.iterrows()):\n",
    "        course_vectors[row[0]] = vectors[i]\n",
    "\n",
    "    hitrate, f1, recall, precision, count = 0, 0, 0, 0, 0\n",
    "    for user_id in users:\n",
    "        recommendations = make_recommendations(user_id, n_recommendations, model, course_vectors, category)\n",
    "\n",
    "        # For metric calculating\n",
    "        user_reviewed_courses = test_df[test_df['reviewers'] == user_id]['course_id'].unique()\n",
    "        res = len(set(user_reviewed_courses) & set(recommendations))\n",
    "\n",
    "        if res > 0:\n",
    "            hitrate += 1\n",
    "\n",
    "        all_courses = np.concatenate((recommendations, user_reviewed_courses))\n",
    "        recommended_vector = [1 if course in recommendations else 0 for course in all_courses]\n",
    "        taken_vector = [1 if course in user_reviewed_courses else 0 for course in all_courses]\n",
    "\n",
    "        # Calculate Metrics\n",
    "        f1 += f1_score(taken_vector, recommended_vector)\n",
    "        precision += precision_score(taken_vector, recommended_vector)\n",
    "        recall += recall_score(taken_vector, recommended_vector)\n",
    "\n",
    "        count +=1\n",
    "    print(f\"Model: {model.__class__.__name__} Features: {n_features}, duration: {(time.time()-start_time):.3f}\")\n",
    "    print(f\"Hit-rate: {(hitrate / count):.3f}, F1: {(f1 / count):.3f}, Precision: {(precision / count):.3f}, Recall: {(recall / count):.3f}\")\n",
    "evaluate_model(30, 10, Doc2Vec, 0000, 'description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "pipeline_tag: sentence-similarity\n",
      "license: apache-2.0\n",
      "tags:\n",
      "- sentence-transformers\n",
      "- feature-extraction\n",
      "- sentence-similarity\n",
      "- transformers\n",
      "---\n",
      "\n",
      "# sentence-transformers/paraphrase-distilroberta-base-v1\n",
      "\n",
      "This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
      "\n",
      "\n",
      "\n",
      "## Usage (Sentence-Transformers)\n",
      "\n",
      "Using this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n",
      "\n",
      "```\n",
      "pip install -U sentence-transformers\n",
      "```\n",
      "\n",
      "Then you can use the model like this:\n",
      "\n",
      "```python\n",
      "from sentence_transformers import SentenceTransformer\n",
      "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
      "\n",
      "model = SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v1')\n",
      "embeddings = model.encode(sentences)\n",
      "print(embeddings)\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "## Usage (HuggingFace Transformers)\n",
      "Without [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n",
      "\n",
      "```python\n",
      "from transformers import AutoTokenizer, AutoModel\n",
      "import torch\n",
      "\n",
      "\n",
      "#Mean Pooling - Take attention mask into account for correct averaging\n",
      "def mean_pooling(model_output, attention_mask):\n",
      "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
      "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
      "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
      "\n",
      "\n",
      "# Sentences we want sentence embeddings for\n",
      "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
      "\n",
      "# Load model from HuggingFace Hub\n",
      "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-distilroberta-base-v1')\n",
      "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-distilroberta-base-v1')\n",
      "\n",
      "# Tokenize sentences\n",
      "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
      "\n",
      "# Compute token embeddings\n",
      "with torch.no_grad():\n",
      "    model_output = model(**encoded_input)\n",
      "\n",
      "# Perform pooling. In this case, max pooling.\n",
      "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
      "\n",
      "print(\"Sentence embeddings:\")\n",
      "print(sentence_embeddings)\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "## Evaluation Results\n",
      "\n",
      "\n",
      "\n",
      "For an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/paraphrase-distilroberta-base-v1)\n",
      "\n",
      "\n",
      "\n",
      "## Full Model Architecture\n",
      "```\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: RobertaModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      ")\n",
      "```\n",
      "\n",
      "## Citing & Authors\n",
      "\n",
      "This model was trained by [sentence-transformers](https://www.sbert.net/). \n",
      "        \n",
      "If you find this model helpful, feel free to cite our publication [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084):\n",
      "```bibtex \n",
      "@inproceedings{reimers-2019-sentence-bert,\n",
      "    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n",
      "    author = \"Reimers, Nils and Gurevych, Iryna\",\n",
      "    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n",
      "    month = \"11\",\n",
      "    year = \"2019\",\n",
      "    publisher = \"Association for Computational Linguistics\",\n",
      "    url = \"http://arxiv.org/abs/1908.10084\",\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SentenceTransformer' object has no attribute 'save_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Development\\School-UNSW\\COMP9727\\project\\comp9727-project\\notebooks\\content-based-rec.ipynb Cell 8\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Development/School-UNSW/COMP9727/project/comp9727-project/notebooks/content-based-rec.ipynb#X20sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m#print(model._model_card_text)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Development/School-UNSW/COMP9727/project/comp9727-project/notebooks/content-based-rec.ipynb#X20sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39m_model_card_text)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Development/School-UNSW/COMP9727/project/comp9727-project/notebooks/content-based-rec.ipynb#X20sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m model_path \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49msave_path\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Development/School-UNSW/COMP9727/project/comp9727-project/notebooks/content-based-rec.ipynb#X20sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m model_name \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(model_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Development/School-UNSW/COMP9727/project/comp9727-project/notebooks/content-based-rec.ipynb#X20sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPre-trained model name:\u001b[39m\u001b[39m\"\u001b[39m, model_name)\n",
      "File \u001b[1;32mc:\\Users\\Timia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1694\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1695\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SentenceTransformer' object has no attribute 'save_path'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def make_recommendations(user_id, n_recommendations, model, course_vectors, category):\n",
    "    user_reviews = train_df[train_df['reviewers'] == user_id][category]\n",
    "    user_vector = model.encode(user_reviews.tolist())\n",
    "\n",
    "    most_similar_courses = find_most_similar_courses(user_vector, course_vectors, user_reviews, category)[:n_recommendations]\n",
    "\n",
    "    recommended_courses = [course_id for course_id, _ in most_similar_courses]\n",
    "\n",
    "    return recommended_courses\n",
    "\n",
    "\n",
    "def find_most_similar_courses(user_vector, course_vectors, user_reviewed_courses, category):\n",
    "    most_similar_courses = []\n",
    "    for other_course_id in courses.index:\n",
    "        if other_course_id in user_reviewed_courses:\n",
    "            continue\n",
    "\n",
    "        desc = train_df[train_df['course_id'] == other_course_id][category]\n",
    "        if desc.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        desc = desc.iloc[0]\n",
    "        course_vector = course_vectors[other_course_id]  # Use precomputed vectors\n",
    "        similarity = cosine_similarity(user_vector, [course_vector])[0][0]  # Remove the extra list\n",
    "        most_similar_courses.append((other_course_id, similarity))\n",
    "\n",
    "    most_similar_courses.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return most_similar_courses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(n_users, n_recommendations, model_name, category):\n",
    "    grouped_df = train_df.groupby('reviewers')['course_id'].count()\n",
    "    filtered_df = grouped_df[grouped_df > 3]\n",
    "    users = filtered_df.index[:n_users]\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "    course_vectors = {}\n",
    "    for id, row in courses.iterrows():\n",
    "        course_vectors[id] = model.encode([row[category]])[0]\n",
    "\n",
    "    hitrate, f1, recall, precision, count = 0, 0, 0, 0, 0\n",
    "    for user_id in users:\n",
    "        recommendations = make_recommendations(user_id, n_recommendations, model, course_vectors, category)\n",
    "\n",
    "        # For metric calculating\n",
    "        user_reviewed_courses = test_df[test_df['reviewers'] == user_id]['course_id'].unique()\n",
    "        res = len(set(user_reviewed_courses) & set(recommendations))\n",
    "\n",
    "        if res > 0:\n",
    "            hitrate += 1\n",
    "\n",
    "        all_courses = np.concatenate((recommendations, user_reviewed_courses))\n",
    "        recommended_vector = [1 if course in recommendations else 0 for course in all_courses]\n",
    "        taken_vector = [1 if course in user_reviewed_courses else 0 for course in all_courses]\n",
    "\n",
    "        # Calculate Metrics\n",
    "        f1 += f1_score(taken_vector, recommended_vector)\n",
    "        precision += precision_score(taken_vector, recommended_vector)\n",
    "        recall += recall_score(taken_vector, recommended_vector)\n",
    "\n",
    "        count +=1\n",
    "    print(f\"Model: {model.__class__.__name__} Features: {n_features}\")\n",
    "    print(f\"Hit-rate: {(hitrate / count):.3f}, F1: {(f1 / count):.3f}, Precision: {(precision / count):.3f}, Recall: {(recall / count):.3f}\")\n",
    "\n",
    "\n",
    "# evaluate_model(30, 10, SentenceTransformer('paraphrase-distilroberta-base-v1'),'description')\n",
    "# evaluate_model(30, 10, SentenceTransformer('paraphrase-distilroberta-base-v1'),'description')\n",
    "# evaluate_model(30, 10, SentenceTransformer('paraphrase-distilroberta-base-v1'),'description')\n",
    "# evaluate_model(30, 10, SentenceTransformer('paraphrase-distilroberta-base-v1'),'description')\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "#print(model._model_card_text)\n",
    "print(model._model_card_text)\n",
    "model_path = model.save_path\n",
    "model_name = os.path.basename(model_path)\n",
    "print(\"Pre-trained model name:\", model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
