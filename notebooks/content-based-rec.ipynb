{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import time\n",
    "wd = %pwd\n",
    "if wd.split('\\\\')[-1] == 'notebooks':\n",
    "    %cd ..\n",
    "\n",
    "from coursemate.dataset import Dataset\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Coursera courses...\n",
      "Loading Coursera reviews...\n",
      "Segmenting out students with less than 3 or more than 50 reviews...\n",
      "Setting the train-test split by rating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "174219it [00:12, 14431.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the training and test rating matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "128771it [00:08, 14608.17it/s]\n",
      "45448it [00:03, 13238.75it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset('data/Coursera_courses.csv', 'data/Coursera.csv', 'data/Coursera_reviews.csv')\n",
    "dataset.set_interaction_counts(3, 50)\n",
    "dataset.set_train_test_split_by_ratings(ratio=0.8)\n",
    "training_matrix, test_matrix = dataset.get_train_test_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dataset.train_ratings.merge(dataset.df_courses, how='left',on='course_id')\n",
    "test_df = dataset.test_ratings.merge(dataset.df_courses, how='left',on='course_id')\n",
    "users = dataset.student_set.copy(deep=True)\n",
    "courses = dataset.course_set.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128771 45448\n",
      "30719 30719\n",
      "30719 468\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df), len(test_df))\n",
    "print(len(pd.unique(train_df['reviewers'])),len(pd.unique(test_df['reviewers'])))\n",
    "print(len(users), len(courses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def process_skills(skill_text):\n",
    "    skills = set(skill_text.replace(')','').replace('(','').replace('-',' ').lower().split())\n",
    "    return ' '.join(skills)\n",
    "\n",
    "def process_description(description):\n",
    "    description = description.lower()\n",
    "    description = re.sub(r'[^\\w\\s]', '', description)\n",
    "    tokens = word_tokenize(description)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    #tokens = [ps.stem(word) for word in tokens]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def process_reviewers(reviewer):\n",
    "    reviewer = reviewer.lower()\n",
    "    reviewer = re.sub(r'[^\\w\\s]', '', reviewer).replace('by ', '').strip()\n",
    "    return reviewer\n",
    "\n",
    "train_df['reviewers'] = train_df['reviewers'].apply(process_reviewers)\n",
    "test_df['reviewers'] = test_df['reviewers'].apply(process_reviewers)\n",
    "\n",
    "train_df['skills'] = train_df['skills'].apply(process_skills)\n",
    "test_df['skills'] = test_df['skills'].apply(process_skills)\n",
    "\n",
    "train_df['description'] = train_df['description'].apply(process_description)\n",
    "test_df['description'] = test_df['description'].apply(process_description)\n",
    "\n",
    "courses['description'] = courses['description'].apply(process_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recommendations(user_id,vectorizer,course_vectors):\n",
    "    #user_reviews = train_df[train_df['reviewers'] == user_id]['course_id'].unique()\n",
    "    #user_vector = vectorizer.transform(train_df[train_df['course_id'].isin(user_reviewed_courses)]['description'])\n",
    "    user_reviews = train_df[train_df['reviewers'] == user_id]['description']\n",
    "    user_vector = vectorizer.transform(user_reviews)\n",
    "\n",
    "    s = time.time()\n",
    "    most_similar_courses = find_most_similar_courses(user_vector,course_vectors,user_reviews)\n",
    "    \n",
    "    recommended_courses = []\n",
    "    for course_id, similarity in most_similar_courses:\n",
    "        recommended_courses.append(course_id)\n",
    "\n",
    "    return recommended_courses\n",
    "\n",
    "\n",
    "def find_most_similar_courses(user_vector,course_vectors,user_reviewed_courses):   \n",
    "    most_similar_courses = []\n",
    "    for other_course_id in courses.index:\n",
    "        if other_course_id in user_reviewed_courses:\n",
    "            continue\n",
    "\n",
    "        desc = train_df[train_df['course_id'] == other_course_id]['description']\n",
    "        if desc.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        desc = desc.iloc[0]\n",
    "        course_vector = course_vectors[other_course_id]\n",
    "        similarity = cosine_similarity(user_vector, course_vector)\n",
    "        most_similar_courses.append((other_course_id, similarity.mean()))\n",
    "        \n",
    "    most_similar_courses.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return most_similar_courses[:5]\n",
    "\n",
    "def evaluate_model(n_users,Vectorizer,n_features):\n",
    "    grouped_df = train_df.groupby('reviewers')['course_id'].count()\n",
    "    filtered_df = grouped_df[grouped_df > 3]\n",
    "    users = filtered_df.index[:n_users]      \n",
    "    if isinstance(n_features, int):\n",
    "        n_features = [n_features]\n",
    "\n",
    "    for n in n_features:\n",
    "        t = time.time()\n",
    "        vectorizer = Vectorizer(max_features=n)\n",
    "        vectorizer.fit(courses['description'])\n",
    "        #print(f\"Fitting {(time.time() - t):.3f}\")\n",
    "        #course_vectors1 = {course_id: vectorizer.transform([desc]) for course_id, desc in zip(train_df['course_id'], train_df['description'])}\n",
    "        \n",
    "        course_vectors = {}\n",
    "        for id,row in courses.iterrows():\n",
    "            course_vectors[id] = vectorizer.transform([row['description']])\n",
    "        # for i in course_vectors.values():\n",
    "        #     for j in course_vectors1.values():\n",
    "        #         if course\n",
    "            #if i not in course_vectors1.values:\n",
    "                #print(i)\n",
    "                \n",
    "\n",
    "        # print(course_vectors == course_vectors1)\n",
    "        # print(f\"Transform course vectors {(time.time() - t):.3f}\")\n",
    "        hitrate,f1,recall,precision,count = 0,0,0,0,0\n",
    "        start_time = time.time()\n",
    "        for user_id in users:\n",
    "            recommendations = make_recommendations(user_id,vectorizer,course_vectors)\n",
    "\n",
    "            user_reviewed_courses = test_df[test_df['reviewers'] == user_id]['course_id'].unique()\n",
    "            res = len(set(user_reviewed_courses) & set(recommendations))\n",
    "\n",
    "            if res > 0:\n",
    "                hitrate += 1\n",
    "\n",
    "            all_courses = np.concatenate((recommendations, user_reviewed_courses))\n",
    "\n",
    "            # Create binary vectors\n",
    "            recommended_vector = [1 if course in recommendations else 0 for course in all_courses]\n",
    "            taken_vector = [1 if course in user_reviewed_courses else 0 for course in all_courses]\n",
    "\n",
    "\n",
    "            # Calculate F1 score\n",
    "            f1 += f1_score(taken_vector, recommended_vector)\n",
    "            precision += precision_score(taken_vector, recommended_vector)\n",
    "            recall += recall_score(taken_vector, recommended_vector)\n",
    "\n",
    "            count +=1\n",
    "        print(f\"Vectorizer: {Vectorizer.__name__} Features: {n}, duration: {(time.time()-start_time):.3f}\")\n",
    "        print(f\"Hit-rate: {(hitrate / count):.3f}, F1: {(f1 / count):.3f}, Precision: {(precision / count):.3f}, Recall: {(recall / count):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 0.080\n",
      "remote-team-management\n",
      "social-media-marketing-introduction\n",
      "attention-models-in-nlp\n",
      "introduction-to-data-analytics\n",
      "covid-19-contact-tracing-for-nursing-professionals\n",
      "building-modern-python-applications-on-aws\n",
      "the-business-of-product-management-one\n",
      "False\n",
      "Transform course vectors 27.638\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(30,CountVectorizer,[1000])\n",
    "#evaluate_model(30,TfidfVectorizer,[100,500,1000,5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.499151706695557\n",
      "0.6956300735473633\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "desc = courses['description']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(desc)\n",
    "print(time.time()-start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(courses['description'])\n",
    "course_vectors = {}\n",
    "for id,row in courses.iterrows():\n",
    "    course_vectors[id] = vectorizer.transform([row['description']])\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468 468\n",
      "<class 'numpy.ndarray'> <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings), len(course_vectors))\n",
    "print(type(embeddings), type(course_vectors))\n",
    "# for i in len(courses):\n",
    "#     print(cosine_similarity(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
